{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "df2d1efd-b550-4785-94b8-f49381985259"
    }
   },
   "source": [
    "# Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi everyone, <br />\n",
    "\n",
    "This session is about text mining. \n",
    "\n",
    "It will walk you though the following sections:\n",
    "\n",
    "1. Text pre-processing\n",
    "2. Term Frequency analysis (TF)\n",
    "3. Inverse Document Frequency (IDF)\n",
    "4. Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "5. Text classification\n",
    "6. Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b9c7083f-551a-4db3-89cd-4237a8d39f02"
    }
   },
   "source": [
    "** Before Starting, let us import some basic text mining tools**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*- \n",
    "# Basic imports\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "import collections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "\n",
    "# Natural Language Tool Kit (NLTK) imports\n",
    "import nltk\n",
    "from nltk.data  import load\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Machine Learning Library (sklearn) imports\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instantiate objects from NLTK\n",
    "sentence_splitter = load('tokenizers/punkt/english.pickle')\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a235458d-1713-4078-bfc1-8bd5e25792ac"
    }
   },
   "source": [
    "For this text mining session, we will use **reviews from Amazon**. These reviews correspond to product reviews from 4 product types \n",
    "* Books\n",
    "* DVD\n",
    "* Electronics\n",
    "* Kitchen Appliances\n",
    "\n",
    "For each review, we know the sentiment associated to it.\n",
    "\n",
    "**Book** \n",
    "\n",
    "\"*What a waste of time. This was like sitting through a very boring business course.*\"\n",
    "\n",
    "\"*An excellent, well explained art book, with beautiful and easy to follow illustrations. The book is a treasure chest of ideas suitable for the primary classroom. This book provides plenty of opportunities to explore the various strands of the visual arts field. A great resource for any teacher, parent or doting aunt*\"\n",
    "\n",
    "**DVD** \n",
    "\n",
    "\"*The sound on this DVD is absolutely horrible.  The dialogue is at a much lower volume than the music and sound effects, making it impossible to view without constantly tinkering.  I also have the VHS, on which the sound is perfect, so I can still watch this wonderful movie.  But I would sure like to get my money back for the DVD*\"\n",
    "\n",
    "**Electronics** \n",
    "\n",
    "\"*Terrible Design did not fit my car's electical outlet, it does not work with my Jeep Liberty.  The jack construction is defective does not even power on*\"\n",
    "\n",
    "**Kitchen appliances** \n",
    "\n",
    "\"*Great blender! I use it daily to make smoothies and it never fails. Powerful motor purees frozen fruits great!! Simple--only two speeds. Easy clean-up *\"\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "7368529c-d2a7-43c9-bfe7-b1d9b46af228"
    }
   },
   "source": [
    "## 1. Text pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5436de00-1b1a-491c-8b12-35eec3be7921"
    }
   },
   "source": [
    "### 1.1  Split into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "b1c5320b-98b0-4651-8ca0-49e7978d305d"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'What a waste of time.'\n",
      "'This was like sitting through a very boring business course.'\n"
     ]
    }
   ],
   "source": [
    "review = \"\"\"What a waste of time. This was like sitting through a very boring business course.\"\"\"\n",
    "for sentence in sentence_splitter.tokenize(review):\n",
    "    pprint(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e35ea6f8-9c58-4b44-86ce-69b3c3021a70"
    }
   },
   "source": [
    "### 1.2 Split sentence into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "d8389243-ae91-4bc3-a53b-d8b421f52081"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'This'\n",
      "'was'\n",
      "'like'\n",
      "'sitting'\n",
      "'through'\n",
      "'a'\n",
      "'very'\n",
      "'boring'\n",
      "'business'\n",
      "'course'\n",
      "'.'\n"
     ]
    }
   ],
   "source": [
    "sentence = \"\"\"This was like sitting through a very boring business course.\"\"\"\n",
    "for token in tokenizer.tokenize(sentence):\n",
    "    pprint(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "f5dcad0f-2ff1-4fe8-8c10-9e73607833d0"
    }
   },
   "source": [
    "### 1.3 Convert tokens to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "6da034ec-588a-4aa1-bdcc-d87070467e3c"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'this'\n",
      "'was'\n",
      "'like'\n",
      "'sitting'\n",
      "'through'\n",
      "'a'\n",
      "'very'\n",
      "'boring'\n",
      "'business'\n",
      "'course'\n",
      "'.'\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentence = ['This','was','like','sitting','through','a','very','boring','business','course','.']\n",
    "for token in tokenized_sentence:\n",
    "    token = token.lower()\n",
    "    pprint(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2dacca0c-8417-490f-b795-40dccc2ba485"
    }
   },
   "source": [
    "### 1.4 Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "9aa859da-a09c-42cc-a654-8508a5450b25"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'this'\n",
      "'was'\n",
      "'like'\n",
      "'sitting'\n",
      "'through'\n",
      "'a'\n",
      "'very'\n",
      "'boring'\n",
      "'business'\n",
      "'course'\n"
     ]
    }
   ],
   "source": [
    "punctuation = set([\",\", \".\", \";\", \"/\", \":\", \"-\", \"--\" ,\"!\", \"?\", \"(\", \")\",\"'\",'\"',\"''\", \"``\"])\n",
    "tokenized_sentence = ['this','was','like','sitting','through','a','very','boring','business','course','.']\n",
    "for token in tokenized_sentence:\n",
    "    if token not in punctuation:\n",
    "        pprint(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0972ea08-a612-478c-9a0b-599dc7a31b74"
    }
   },
   "source": [
    "### 1.5 Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "6188ff57-9eb6-41d9-8e59-0950d6410a89"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([u'a',\n",
      "     u'about',\n",
      "     u'above',\n",
      "     u'after',\n",
      "     u'again',\n",
      "     u'against',\n",
      "     u'ain',\n",
      "     u'all',\n",
      "     u'am',\n",
      "     u'an',\n",
      "     u'and',\n",
      "     u'any',\n",
      "     u'are',\n",
      "     u'aren',\n",
      "     u'as',\n",
      "     u'at',\n",
      "     u'be',\n",
      "     u'because',\n",
      "     u'been',\n",
      "     u'before',\n",
      "     u'being',\n",
      "     u'below',\n",
      "     u'between',\n",
      "     u'both',\n",
      "     u'but',\n",
      "     u'by',\n",
      "     u'can',\n",
      "     u'couldn',\n",
      "     u'd',\n",
      "     u'did',\n",
      "     u'didn',\n",
      "     u'do',\n",
      "     u'does',\n",
      "     u'doesn',\n",
      "     u'doing',\n",
      "     u'don',\n",
      "     u'down',\n",
      "     u'during',\n",
      "     u'each',\n",
      "     u'few',\n",
      "     u'for',\n",
      "     u'from',\n",
      "     u'further',\n",
      "     u'had',\n",
      "     u'hadn',\n",
      "     u'has',\n",
      "     u'hasn',\n",
      "     u'have',\n",
      "     u'haven',\n",
      "     u'having',\n",
      "     u'he',\n",
      "     u'her',\n",
      "     u'here',\n",
      "     u'hers',\n",
      "     u'herself',\n",
      "     u'him',\n",
      "     u'himself',\n",
      "     u'his',\n",
      "     u'how',\n",
      "     u'i',\n",
      "     u'if',\n",
      "     u'in',\n",
      "     u'into',\n",
      "     u'is',\n",
      "     u'isn',\n",
      "     u'it',\n",
      "     u'its',\n",
      "     u'itself',\n",
      "     u'just',\n",
      "     u'll',\n",
      "     u'm',\n",
      "     u'ma',\n",
      "     u'me',\n",
      "     u'mightn',\n",
      "     u'more',\n",
      "     u'most',\n",
      "     u'mustn',\n",
      "     u'my',\n",
      "     u'myself',\n",
      "     u'needn',\n",
      "     u'no',\n",
      "     u'nor',\n",
      "     u'not',\n",
      "     u'now',\n",
      "     u'o',\n",
      "     u'of',\n",
      "     u'off',\n",
      "     u'on',\n",
      "     u'once',\n",
      "     u'only',\n",
      "     u'or',\n",
      "     u'other',\n",
      "     u'our',\n",
      "     u'ours',\n",
      "     u'ourselves',\n",
      "     u'out',\n",
      "     u'over',\n",
      "     u'own',\n",
      "     u're',\n",
      "     u's',\n",
      "     u'same',\n",
      "     u'shan',\n",
      "     u'she',\n",
      "     u'should',\n",
      "     u'shouldn',\n",
      "     u'so',\n",
      "     u'some',\n",
      "     u'such',\n",
      "     u't',\n",
      "     u'than',\n",
      "     u'that',\n",
      "     u'the',\n",
      "     u'their',\n",
      "     u'theirs',\n",
      "     u'them',\n",
      "     u'themselves',\n",
      "     u'then',\n",
      "     u'there',\n",
      "     u'these',\n",
      "     u'they',\n",
      "     u'this',\n",
      "     u'those',\n",
      "     u'through',\n",
      "     u'to',\n",
      "     u'too',\n",
      "     u'under',\n",
      "     u'until',\n",
      "     u'up',\n",
      "     u've',\n",
      "     u'very',\n",
      "     u'was',\n",
      "     u'wasn',\n",
      "     u'we',\n",
      "     u'were',\n",
      "     u'weren',\n",
      "     u'what',\n",
      "     u'when',\n",
      "     u'where',\n",
      "     u'which',\n",
      "     u'while',\n",
      "     u'who',\n",
      "     u'whom',\n",
      "     u'why',\n",
      "     u'will',\n",
      "     u'with',\n",
      "     u'won',\n",
      "     u'wouldn',\n",
      "     u'y',\n",
      "     u'you',\n",
      "     u'your',\n",
      "     u'yours',\n",
      "     u'yourself',\n",
      "     u'yourselves'])\n"
     ]
    }
   ],
   "source": [
    "stopwords_set = set(stopwords.words('english'))\n",
    "pprint(stopwords_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "8665b462-14cf-4396-9d95-48883633a415"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'like'\n",
      "'sitting'\n",
      "'boring'\n",
      "'business'\n",
      "'course'\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentence = ['this','was','like','sitting','through','a','very','boring','business','course']\n",
    "for token in tokenized_sentence:\n",
    "    if token not in stopwords_set:\n",
    "        pprint(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "82ef5c04-51be-4c7b-97c4-bb64cb0be33a"
    }
   },
   "source": [
    "### 1.6 Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "5452ad90-d368-49e5-bbf2-f244adcaf011"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'like'\n",
      "u'sit'\n",
      "u'bore'\n",
      "u'busi'\n",
      "u'cours'\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentence = ['like','sitting','boring','business','course']\n",
    "for token in tokenized_sentence:\n",
    "    stem = stemmer.stem(token)\n",
    "    pprint(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5c74cf28-3b01-4ff9-9bac-3d7735d18579"
    }
   },
   "source": [
    "### 1.7 All together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "6358b68b-e287-4dfc-bade-47d5d7ed9ec8"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'excel',\n",
      " 'well',\n",
      " u'explain',\n",
      " 'art',\n",
      " 'book',\n",
      " u'beauti',\n",
      " u'easi',\n",
      " 'follow',\n",
      " u'illustr',\n",
      " 'book',\n",
      " u'treasur',\n",
      " 'chest',\n",
      " u'idea',\n",
      " u'suitabl',\n",
      " u'primari',\n",
      " 'classroom',\n",
      " 'book',\n",
      " u'provid',\n",
      " u'plenti',\n",
      " u'opportun',\n",
      " u'explor',\n",
      " u'variou',\n",
      " u'strand',\n",
      " 'visual',\n",
      " u'art',\n",
      " 'field',\n",
      " 'great',\n",
      " u'resourc',\n",
      " 'teacher',\n",
      " 'parent',\n",
      " u'dote',\n",
      " 'aunt']\n"
     ]
    }
   ],
   "source": [
    "# Preprocess a given text\n",
    "def preprocess_text(review):\n",
    "    tokens = []\n",
    "    # 1. Split into sentences\n",
    "    for sentence in sentence_splitter.tokenize(review):\n",
    "        # 2. Split into tokens\n",
    "        for token in tokenizer.tokenize(sentence):\n",
    "            token = token.lower()\n",
    "            # 3. Filter on stoplist and punctuation\n",
    "            if token not in stopwords_set and token not in punctuation:\n",
    "                # 4. Stemming (takes root)\n",
    "                stem = stemmer.stem(token)\n",
    "                tokens.append(stem)\n",
    "    return tokens\n",
    "\n",
    "pprint( preprocess_text(\"\"\"An excellent, well explained art book, with beautiful and easy to follow illustrations. \n",
    "The book is a treasure chest of ideas suitable for the primary classroom. \n",
    "This book provides plenty of opportunities to explore the various strands of the visual arts field. \n",
    "A great resource for any teacher, parent or doting aunt\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Term Frequency analysis (TF)\n",
    "\n",
    "A central question in text mining and natural language processing is how to quantify what a document is about.\n",
    "\n",
    "One measure of how important a word may be is its **term frequency** (tf), how often a word occurs in a document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Without pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "' Sphere by Michael Crichton is an excellant novel. This was certainly the hardest to put down of all of the Crichton novels that I have read.   The story revolves around a man named Norman Johnson. Johnson is a phycologist. He travels with 4 other civilans to a remote location in the Pacific Ocean to help the Navy in a top secret misssion. They quickly learn that under the ocean is a half mile long spaceship. The civilans travel to a center 1000 feet under the ocean to live while researching the spacecraft. They are joined by 5 Navy personel to help them run operations. However on the surface a typhoon comes and the support ships on the surface must leave. The team of ten is stuck 1000 feet under the surface of the ocean. After a day under the sea they find out that the spacecraft is actually an American ship that has explored black holes and has brought back some strange things back to earth.  This novel does not have the research that some of the other Crichton novels have, but it still has a lot of information on random things from the lawes of partial pressure to behavior analysis.  I would strongly recommend this book \\n'\n",
      "------\n",
      "' Dr. Oz is an accomplished heart surgeon in the field of cardiac transplantation. He describes how he combines complementary medicine (e.g. hypnosis, reflexology, yoga, message, acupuncture. Etc) with orthodox Western medicine. There is an excellent forward by Dr Dean Ornish, and an interesting epilogue containing an overview of the complementary medicine techniques. The bulk of the book contains stories of patients Dr. Oz treated using this revolutionary way. I am a cardiologist, and I have a great interest in combining western medicine with complementary medicine, which is the reason I bought this book. However this book was a bit boring to read and was also a bit of a disappointment. Nevertheless, those interested in this new medicine, which I think will be the medicine of the new millennium, will want to read this book \\n'\n",
      "------\n",
      "' The most gorgeous artwork in comic books. Contains the most extraordinary artwork of Alex Ross(Superman, Batman, Wonder Woman,  the Justice League, etc, even Hanna-Barbera!) A comics fan, I recieved this as a Christmas gift, and I read it again and again. A must-have for comic book fans. \\n'\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "# Some previews of reviews\n",
    "with open('data/sorted_data_acl/books/positive_text.review', 'r') as myfile:\n",
    "    pos_books = myfile.readlines()\n",
    "    \n",
    "    print'------'\n",
    "    pprint(pos_books[0])\n",
    "    print'------'\n",
    "    pprint(pos_books[1])\n",
    "    print'------'\n",
    "    pprint(pos_books[2])\n",
    "    print'------'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge book reviews together\n",
    "with open('data/sorted_data_acl/books/positive_text.review', 'r') as myfile:\n",
    "    pos_book = myfile.read().decode(\"utf-8\")\n",
    "with open('data/sorted_data_acl/books/negative_text.review', 'r') as myfile:\n",
    "    neg_book = myfile.read().decode(\"utf-8\")   \n",
    "book_reviews = pos_book + neg_book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'the', 16659),\n",
      " (u',', 16220),\n",
      " (u'of', 10141),\n",
      " (u'and', 9703),\n",
      " (u'to', 8850),\n",
      " (u'a', 8254),\n",
      " (u'I', 6058),\n",
      " (u'is', 5984),\n",
      " (u'in', 5044),\n",
      " (u'that', 4364),\n",
      " (u'book', 3689),\n",
      " (u'this', 3605),\n",
      " (u'it', 3414),\n",
      " (u\"'s\", 3048),\n",
      " (u'for', 2944)]\n"
     ]
    }
   ],
   "source": [
    "# Split into words, without further pre-processing\n",
    "tokens = tokenizer.tokenize(book_reviews)\n",
    "\n",
    "# count frequency of words\n",
    "counter = collections.Counter(tokens)\n",
    "pprint(counter.most_common(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DVD reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'the', 16426),\n",
      " (u',', 15723),\n",
      " (u'and', 9387),\n",
      " (u'a', 8236),\n",
      " (u'of', 7917),\n",
      " (u'to', 7443),\n",
      " (u'is', 6149),\n",
      " (u'I', 5580),\n",
      " (u'in', 4529),\n",
      " (u'it', 4011),\n",
      " (u'that', 3810),\n",
      " (u'this', 3656),\n",
      " (u\"'s\", 3517),\n",
      " (u'was', 2637),\n",
      " (u'for', 2609)]\n"
     ]
    }
   ],
   "source": [
    "# Merge dvd reviews together\n",
    "with open('data/sorted_data_acl/dvd/positive_text.review', 'r') as myfile:\n",
    "    pos_dvd = myfile.read().decode(\"utf-8\")\n",
    "with open('data/sorted_data_acl/dvd/negative_text.review', 'r') as myfile:\n",
    "    neg_dvd = myfile.read().decode(\"utf-8\")\n",
    "dvd_reviews = pos_dvd + neg_dvd\n",
    "    \n",
    "# Split into words, without further pre-processing\n",
    "tokens = tokenizer.tokenize(dvd_reviews)\n",
    "\n",
    "# count frequency of words\n",
    "counter=collections.Counter(tokens)\n",
    "pprint(counter.most_common(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, many common words which do not tell us much about our reviews, are present.\n",
    "\n",
    "Pre-processing allows us to remove some of the highly frequent common words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 With pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Book reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'book', 4933),\n",
      " (u'read', 1880),\n",
      " (u'one', 1497),\n",
      " (u'like', 1062),\n",
      " (u'time', 865),\n",
      " (u'would', 845),\n",
      " (u'stori', 801),\n",
      " (u'get', 707),\n",
      " (u'work', 691),\n",
      " (u'good', 677),\n",
      " (u'make', 672),\n",
      " (u'use', 661),\n",
      " (u'author', 638),\n",
      " (u'much', 636),\n",
      " (u'mani', 631)]\n"
     ]
    }
   ],
   "source": [
    "# pre-processing of reviews\n",
    "book_reviews_prepro = preprocess_text(book_reviews.replace(\"'\",' '))\n",
    "\n",
    "# count frequency of words\n",
    "counter=collections.Counter(book_reviews_prepro)\n",
    "pprint(counter.most_common(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DVD reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'movi', 2763),\n",
      " (u'film', 2100),\n",
      " (u'one', 1589),\n",
      " (u'like', 1268),\n",
      " (u'dvd', 986),\n",
      " (u'get', 922),\n",
      " (u'watch', 904),\n",
      " (u'time', 881),\n",
      " (u'good', 857),\n",
      " (u'make', 777),\n",
      " (u'would', 717),\n",
      " (u'great', 706),\n",
      " (u'see', 702),\n",
      " (u'...', 697),\n",
      " (u'charact', 692)]\n"
     ]
    }
   ],
   "source": [
    "# pre-processing of reviews\n",
    "dvd_reviews_prepro = preprocess_text(dvd_reviews.replace(\"'\",' '))\n",
    "\n",
    "# count frequency of words\n",
    "counter=collections.Counter(dvd_reviews_prepro)\n",
    "pprint(counter.most_common(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Electronics  reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'use', 1399),\n",
      " (u'work', 1155),\n",
      " (u'one', 1013),\n",
      " (u'get', 821),\n",
      " (u'good', 655),\n",
      " (u'sound', 648),\n",
      " (u'would', 645),\n",
      " (u'product', 637),\n",
      " (u'time', 600),\n",
      " (u'great', 569),\n",
      " (u'like', 561),\n",
      " (u'phone', 501),\n",
      " (u'problem', 500),\n",
      " (u'buy', 492),\n",
      " (u'...', 471)]\n"
     ]
    }
   ],
   "source": [
    "# Merge Electronics reviews together\n",
    "with open('data/sorted_data_acl/electronics/positive_text.review', 'r') as myfile:\n",
    "    pos_elec = myfile.read().decode(\"utf-8\")\n",
    "with open('data/sorted_data_acl/electronics/negative_text.review', 'r') as myfile:\n",
    "    neg_elec = myfile.read().decode(\"utf-8\")\n",
    "elec_reviews = pos_elec + neg_elec\n",
    "\n",
    "# pre-processing of reviews\n",
    "elec_reviews_prepro = preprocess_text(elec_reviews.replace(\"'\",' '))\n",
    "\n",
    "# count frequency of word\n",
    "counter=collections.Counter(elec_reviews_prepro)\n",
    "pprint(counter.most_common(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kitchen appliance reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'use', 1505),\n",
      " (u'one', 1005),\n",
      " (u'time', 699),\n",
      " (u'get', 653),\n",
      " (u'make', 621),\n",
      " (u'like', 614),\n",
      " (u'work', 608),\n",
      " (u'pan', 604),\n",
      " (u'would', 580),\n",
      " (u'great', 547),\n",
      " (u'product', 515),\n",
      " (u'set', 475),\n",
      " (u'coffe', 457),\n",
      " (u'well', 457),\n",
      " (u'look', 445)]\n"
     ]
    }
   ],
   "source": [
    "# Merge Kitchen appliance reviews together\n",
    "with open('data/sorted_data_acl/kitchen_&_housewares/positive_text.review', 'r') as myfile:\n",
    "    pos_kitch = myfile.read().decode(\"utf-8\")\n",
    "with open('data/sorted_data_acl/kitchen_&_housewares/negative_text.review', 'r') as myfile:\n",
    "    neg_kitch = myfile.read().decode(\"utf-8\")\n",
    "kitch_reviews = pos_kitch + neg_kitch\n",
    "\n",
    "# pre-processing of reviews\n",
    "kitch_reviews_prepro = preprocess_text(kitch_reviews.replace(\"'\",' '))\n",
    "\n",
    "# count frequency of word\n",
    "counter=collections.Counter(kitch_reviews_prepro)\n",
    "pprint(counter.most_common(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words now better quantify our reviews.\n",
    "\n",
    "However, there are still many **common words** that are not very useful (*one, get, make, even, also, well*), and which are present in all reviews, whetever the category or the sentiment.\n",
    "\n",
    "We would like to extract truly **distinctive keywords** to better characterise our reviews within each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhEAAAFsCAYAAAByyuuQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAHTNJREFUeJzt3X+MXeV95/H3FxIwOCsr7CReh4i0WVbWKNJSmLBbwjqp\nQrvsKpWb4BAyG1eRIFUi0LYaNZtVtSEk2Qpp88ub/EHFSrsyEc1oaTdNrZKCSkoCak1gPRhv2Ykp\nxegSd7AYiKdk8Yx/PfvHvde+c+fOzL3P/XHuufN+SVeeec5zznmGB9sfn/P8iJQSkiRJnbqg6AZI\nkqRyMkRIkqQshghJkpTFECFJkrIYIiRJUhZDhCRJymKIkCRJWQwRkiQpiyFCkiRlMURIkqQshghJ\nkpSl0BAREZdExIsR8ZUi2yFJkjpX9JOI/wTsL7gNkiQpQ2EhIiKuBLYDf15UGyRJUr4in0R8Dfg9\nIApsgyRJytRxiIiIHRGxLyKORsTZiNjZos4dEXEkIk5ExBMRcW3T8Z3A4ZTS8/WivOZLkqSi5DyJ\n2AwcBG4HUvPBiLgF+DpwF3A18AzwcESMNVT7ZeDjEfEC1ScSn4qIz2e0RZIkFSRSWpED2j854izw\n4ZTSvoayJ4Afp5R+p/Z9AC8B30oprZiFERGfBN6TUvrcGvf5x8CNwIvAYnaDJUnaeDYBvwA8nFJ6\ntZcXflMvLxYRbwYmgLvrZSmlFBGPANd1cekbgT/ssnmSJG1knwC+08sL9jREAGPAhcCxpvJjVGdi\nrJBSuq+N674IcP/99zM+Pt5N+3piamqKPXv2DMX1Ojm3nbrr1VnreKtjq9Xv9X/Dbtif7ffnauWj\n3J/dXNP+7F5Z+7Pd+rl92kn57Owsu3fvhtrfpb3U6xDRL4sA4+PjXHPNNUW3hS1btvS0Hd1cr5Nz\n26m7Xp21jrc6tlr9Xv837Ib92X5/rlY+yv3ZzTXtz+6VtT/brZ/bp5l/tvZ8OECvQ8Q8cAbY2lS+\nFXi524tPTU2xZcsWJicnmZyc7PZy2Xp9726u18m57dRdr85ax1sdK7Kf2mV/dnZs2Pu0H+3Lvab9\n2b2y9me79XP7tJ3y6elppqenWVhYWLcduQY1sLJCdWDlVzPvcw1w4MCBA0OTjtWdnTt3sm/fvvUr\nqhTsz9Fif46WmZkZJiYmACZSSjO9vHbHTyIiYjNwJefXdnh3RFwFvJZSegn4BrA3Ig4ATwJTwKXA\n3p60WJIkDYWc1xnvBR6lukZEoromBMB9wK0ppQdqa0J8meprjIPAjSmlV3rQXo2IYX+Eqs7Yn6PF\n/lS7unqdMSj11xnvf//7h2JMhCRJw65xTMRjjz0GfXidUaoQ4ZgISZI6088xEUVvBS5JkkrKECFJ\nkrIYIiRJUpayrFgJDM9iU5IkDbuhX2xqUBxYKUlSHgdWSpKkoWOIkCRJWQwRkiQpiwMrJUkaQQ6s\nrHFgpSRJeRxYKUmShk6pXmf8xm/s4uKLLym6GUPjssveyve+90e84x3vKLopkqQNqFQh4qc/fR/V\n3cUFi/zd3/0B+/fvZ9euXUU3RpK0AZUqRMDvAo6JqPoZ8AdFN0KStIGVLERMAVuAydpHkiS1MojZ\nGSULEXvwSYQkSeurL4fQMDuj55ydIUmSshgiJElSFkOEJEnKYoiQJElZDBGSJCmLIUKSJGUp2RRP\n14mQJKkdrhOxgutESJLUDteJkCRJQ8sQIUmSshgiJElSFkOEJEnKYoiQJElZDBGSJCmLIUKSJGUp\n2ToRLjYlSVI7XGxqBRebkiSpHS42JUmShpYhQpIkZTFESJKkLIYISZKUxRAhSZKyGCIkSVIWQ4Qk\nScpiiJAkSVkMEZIkKYshQpIkZTFESJKkLIYISZKUpWQbcLmLpyRJ7XAXzxXcxVOSpHa4i6ckSRpa\nhghJkpTFECFJkrIYIiRJUhZDhCRJymKIkCRJWQwRkiQpiyFCkiRlMURIkqQshghJkpTFECFJkrIY\nIiRJUhZDhCRJymKIkCRJWQoJERGxJSKeioiZiDgUEZ8qoh2SJCnfmwq67z8AO1JKixFxCfBsRPyv\nlNLPCmqPJEnqUCEhIqWUgMXat5fUfo0i2iJJkvIUNiai9krjIFABvppSeq2otkiSpM51HCIiYkdE\n7IuIoxFxNiJ2tqhzR0QciYgTEfFERFzbXCeltJBS+iXgF4FPRMTb8n4ESZJUhJwnEZuBg8DtQGo+\nGBG3AF8H7gKuBp4BHo6IsVYXSym9UquzI6MtkiSpIB2HiJTSQymlL6SU/pTW4ximgHtTSt9OKf0E\n+AzwBnBrvUJEvD0i3lL7egvwfuBwzg8gSZKK0dOBlRHxZmACuLtellJKEfEIcF1D1XcB/y0ioBpE\nvplSenb9O0wBW5rKJmsfSZI2tunpaaanp5eVLSws9O1+vZ6dMQZcCBxrKj8GbK9/k1J6iuqrjg7t\nAa7Jb50kSSNscnKSycnl/7CemZlhYmKiL/dzxUpJkpSl1yFiHjgDbG0q3wq83ON7SZKkAvX0dUZK\n6VREHABuAPYBRHXgww3At7q/Q31MhOMgJElaS318xFCNiYiIzcCVnJ+Z8e6IuAp4LaX0EvANYG8t\nTDxJ9W/+S4G93TfXMRGSJLWjPj6in2Micp5EvBd4lOoaEYnqmhAA9wG3ppQeqK0J8WWqrzEOAjfW\n1oOQJEkjouMQkVL6EeuMpUgp3QPck9soSZI0/IraxTOTYyIkSWrHUI6JKJZjIiRJascgxkS4ToQk\nScpiiJAkSVlK9jrDMRGSJLXDMRErOCZCkqR2OCZCkiQNLUOEJEnKYoiQJElZSjYmQs1eeeUVZmZm\nBna/sbExrrjiioHdT5I0vEoWIpyd0ey3f3uKU6cWB3a/TZsu5fDhWYOEJA25QczOiJRS3y7eKxFx\nDXAADuDsjLqfAZfVvr4fGB/APWeB3Rw4cIBrrrEfJKkMGmZnTKSUevroumRPItTaOIYrSdKgObBS\nkiRlMURIkqQshghJkpSlZGMinJ0hSVI73DtjBffOkCSpHe6dIUmShpYhQpIkZTFESJKkLIYISZKU\nxRAhSZKylGx2hlM8JUlqh1M8V3CKpyRJ7XCKpyRJGlqGCEmSlMUQIUmSshgiJElSFkOEJEnKYoiQ\nJElZDBGSJClLydaJ0DCYnZ3t+hpjY2NcccUVPWiNJKkoJQsRrlhZrDngAnbv3t31lTZtupTDh2cN\nEpLUJ65YuYIrVhbrOHAWuB8Y7+I6sywu7mZ+ft4QIUl9MogVK0sWIjQcxjHMSZIcWClJkrIYIiRJ\nUhZDhCRJymKIkCRJWQwRkiQpiyFCkiRlMURIkqQshghJkpTFECFJkrIYIiRJUhZDhCRJylKyvTPc\nxVOSpHa4i+cK7uI5SmZnZzuqPzY25q6fktQmd/HUiJoDLmD37t0dnbVp06UcPjxrkJCkIWGIUAGO\nA2eB+6luK96OWRYXdzM/P2+IkKQhYYhQgcbx9ZQklZezMyRJUhZDhCRJymKIkCRJWQwRkiQpiyFC\nkiRlMURIkqQshghJkpTFdSJUKu0ule0S2ZLUf4YIlURnS2W7RLYk9Z8hQiXRyVLZLpEtSYNgiFDJ\nuFS2JA2LQgZWRsQ7I+LRiHg2Ig5GxEeLaIckScpX1JOI08DvpJQORcRW4EBEPJhSOlFQeyRJUocK\neRKRUno5pXSo9vUxYB64rIi2SJKkPIWvExERE8AFKaWjRbdFkiS1r+MQERE7ImJfRByNiLMRsbNF\nnTsi4khEnIiIJyLi2lWudRlwH/BbnTddkiQVKedJxGbgIHA7kJoPRsQtwNeBu4CrgWeAhyNirKne\nRcCfAHenlH6c0Q5JklSgjkNESumhlNIXUkp/CkSLKlPAvSmlb6eUfgJ8BngDuLWp3n3AD1JK3+m0\nDZIkqXg9nZ0REW8GJoC762UppRQRjwDXNdS7HrgZOBQRH6H6ROM3U0rPrn2HKWBLU9lk7SNJ0sY2\nPT3N9PT0srKFhYW+3a/XUzzHgAuBY03lx4Dt9W9SSn+Vd+89uNCQeqFSqTA/P7/qcffekFRGk5OT\nTE4u/4f1zMwMExMTfbmfK1ZqZK22Wdfc3By7dt3M0tLqy5K494Ykra/XIWIeOANsbSrfCrzc/eXr\nrzN8haG1tLtZ12r7cLj3hqTyq7/aKM3rjJTSqYg4ANwA7AOIiKh9/63u7+DrDLVjvc26vg/ciftw\nSBpl9VcbQ/U6IyI2A1dyfmbGuyPiKuC1lNJLwDeAvbUw8STVxweXAnt70mKpbauFhNavOSRJncl5\nEvFe4FGqMyoS1TUhoDpl89aU0gO1NSG+TPU1xkHgxpTSKz1oryRJGhIdh4iU0o9YZ32JlNI9wD25\njVqdYyIkSWpH6cZE9J9jIjQ4rWZ3OPVTUlkM5ZgIafStPrvDqZ+SdF7hu3hKw6dxdseBhs/9LC6+\nseYiVZK0kfgkQlqVU0AlaS0lCxEOrJQkqR0OrFzBgZWSJLVjEAMrHRMhSZKyGCIkSVKWkr3OkIrn\n+hGSVFWyEOHAShXJ9SMklYcDK1dwYKWKtNruoG4dLmn4uGKlNJRcP0KSwIGVkiQpkyFCkiRl8XWG\n1CPNszaWlpa4+OKLl5U5i0PSKClZiHB2hobRarM2LgTOLCtxFoekQXF2xgrOztAwajVr4/vAnU1l\n1Vkcjz/+OOPj1TKfTEjqF2dnSKXSOGtjtkXZyicWPpmQVGYOrJQGpvGJxQHgfhYX32B+fr7YZklS\nJp9ESAPnOhOSRoMhQhoilUpl2ZMJx0xIGmaGCKlg9amhc3Nz7Np1M0tLJ84dc8yEpGFmiJAKs9rU\n0PqMDvfkkDTcShYiXCdCo6R5amh9WqhjJiR1z3UiVnCdCI2iemiYXa+iJLVtEOtEOMVTkiRlMURI\nkqQshghJkpTFECFJkrIYIiRJUhZDhCRJymKIkCRJWUq2ToSLTUmS1A4Xm1rBxaa08dT31nAzLkmd\nGMRiUyULEdJGsnxvjcbNuBp3+zRcSCqKIUIaWo17a3BuMy6A7dvHWVx8A3CnT0nFcWClNPTGa5+q\n+fn5WoC4H7ifxcU3zoULSRokn0RIpTW+fhVJ6iNDhDQCHHwpqQiGCKnUVh98KUn95pgIqdQaB186\nPkLSYPkkQhoJjo+QNHg+iZAkSVl8EiFtIPVFqhyAKakXDBHSBlGpVM4tUuUATEm94OsMaYM4v0jV\n5x2AKaknSvYkwl08pe69q+gGSBoAd/FcwV08JUlqxyB28fR1hiRJylKyJxGS1uMS2JIGxRAhjQyX\nwJY0WIYIqUTqTxlaa1wCGxYXd5+bgTE/P7/OuZLUOUOEVArLnzKs7fwS2HNzc1x//Y7a1E5J6i0H\nVkql0PiU4T+3f9bx47UA0fq8SqXCzMwMlUqlR+2UtJH4JEIqldyNtlae1/iUwvETknL4JELaoM4/\npXAFS0l5DBHShucKlpLyGCKkEXbkyJGsY5LUDkOENJKqsznuvPPOFsdeXeOYJLXPECGNpPpsjtta\nHPv5GsckqX2GCGmkbcs8JknrM0RIkqQshYWIiPhuRLwWEQ8U1QZJkpSvyCcR/xX4zQLvL0mSulBY\niEgpPUZ1hJckSSohl72WtEKlUmF+fp6xsTGXwpa0qo6fRETEjojYFxFHI+JsROxsUeeOiDgSESci\n4omIuLY3zZXUb5VKhe3bx5mYmGD79nE355K0qpzXGZuBg8DtQGo+GBG3AF8H7gKuBp4BHo6IsS7a\nKWlA5ufn3VNDUls6fp2RUnoIeAggIqJFlSng3pTSt2t1PgN8CLgV+EpT3ah9JA2d1ntqNL/qqD+p\n8LWHtPH0dExERLwZmADurpellFJEPAJc11T3L4B/DmyOiApwc0rpx2vfYQrY0lQ2WftI6rf6q476\n9uF/+ZeP8MEP/iqAW4lLQ2B6eprp6ellZQsLC327X68HVo4BFwLHmsqPAdsbC1JKv9b55fcA12Q2\nTVK3lr/q+H1eeOGF2vfVY4YIqViTk5NMTi7/h/XMzAwTExN9uZ8rVkrK4PbhknofIuaBM8DWpvKt\nwMs9vpckSSpQT19npJRORcQB4AZgH5wbfHkD8K3u71AfE+E4CEmS1lIfHzFUYyIiYjNwJednVbw7\nIq4CXkspvQR8A9hbCxNPUv2b/1Jgb/fNdUyEJEntqI+P6OeYiJwnEe8FHqW6RkSiuiYEwH3ArSml\nB2prQnyZ6muMg8CNKaVXetBeSZI0JHLWifgR64ylSCndA9yT2yhJgzc7O8vYWP6acLOzsywtLXH5\n5Zc7S0PaIEq2d4ZjIqTeexW4gN27d7Np06X88R8/kHGN6vlwIRdffBHPPfcTg4RUsEGMiSjZFM89\nVMdrGiCk3vk5cJb6MtfHjx/PuMZZ4DbgDEtLJ1wqWxoCk5OT7Nu3jz179vTtHiULEZL6p9u1H7b1\npBWSysMQISnbkSNHOj6nUqms2Bm0sazVcUnDyTERkjJUx1HceeedHZ1V33sDzu+10VjmXhxS7zgm\nYgXHREjDoT6O4raOzqrvvdG4xXhjWX0vDrcgl7rnmAhJQ85xENJGZoiQJElZDBGSJClLyQZWSuq3\nVjMuKpUK8/PzHDp0qKtrz83NdXW+pOFSshDh7Aypf1rPuJibm+P663ewuPgG3Ty8rFQq3HTTR7tr\noqS2OTtjBWdnSP3TesbF8ePHawHittrxPPPz85w8udhNAyV1wNkZkgqw2owLZ2JIWs4QIUmSshgi\nJElSlpINrJQ0aDn7YzSqz+xonpkxNzfHzMzMsvL1Vqms76nhctjScChZiHB2hjQ4eftjNKrvi7G4\n+AYXXbRp2bGbbrqZkydPNJRfwGc/+x/XvRa4r4bUDmdnrODsDGlw8vbHaFTfFwM+v2JmxsmTJ5rK\nz3L69NK613JfDak9zs6QNAR6MSvjXR2WSyoDQ4QkScpiiJAkSVkMEZIkKYshQlLfdLPhVq8366pU\nKuemiErqjZKFiClgJzBddEMkrWP5hluvAtFwNFqc0egCbrrp5q7/0p+bm+OLX/wiTz31FNu3j7N9\n+3hH16yf7+6jKqPp6Wl27tzJ1NRU3+5RshDhFE+pLJZvuPVzIDUcTS3OaHSWkydPdD2Vc25uji99\n6Us899xzWdND6+cbIlRGTvGUJElDyxAhSZKyGCIkSVIWQ4QkScpiiJAkSVkMEZIkKYshQpIkZXlT\n0Q2QNHpmZ2c5depU365fX7ehvnDUFVdcsWa99cok5SlZiJgCtlBdbMoFp6ThdAG7d++mvQed9ZUs\n11t8qlGwc+eH+d73vsvHPvZxAA4fnj0XJObm5rj33nv50Ic+dG7FzPMLTFVXwvzbv/3JqsFDGhXT\n09NMT0+zsLDQt3uU7HWGK1ZKw+8scFvt1/U0r2TZjsTZs6d55plnWq5C2bhKZX3FzNdff/1c23qx\nEqZUBq5YKamkthXdAEkDYIiQJElZDBGSJCmLIUKSJGUxREiSpCyGCEmSlMUQIUmSshgiJElSFkOE\nJEnKYoiQJElZDBGSJClLyTbgkrSRzM3NsX//fi6//PJ1d9+sVCocPXqUw4cPAyzbH+P48eMt68Pq\nO4A2t6PVOZ1cQxpFJQsR7uIpjbblO3p+5CO7OHXqNBdd9CbSGvt0zc3N8b73Xc/S0kmqG39dwGc/\n+7lzx7/5zW+1qP+vOH36FPv3/zXXXnvtqtet7/z5wx/+gA9+8FeB6q6hANu3j5/7vjlI1HcT/fSn\nP822bdvaPib1irt4ruAuntJoW54UTp1aAs5w8uRS7evWjh8/ztLSIud3Dj3L6dMnzx0/ffpUi/on\nOHPmNM8999ya163v/PnCCy8s2zV0fn6+5S6idfXdRFs9QVnrmNQr7uIpSZKGliFCkiRlMURIkqQs\nhghJkpTFECFJkrIYIiRJUhZDhCRJymKIkCRJWQwRkiQpiyFCkiRlMURIkqQshghJkpTFECFJkrIY\nIiRJUpbCQkRE/HpE/CQiDkfEbUW1Q5Ik5SkkRETEhcDXgV8BrgH+Q0S8tYi2SJKWm56eLroJKomi\nnkT8C+BvUkovp5T+H/B94F8X1BZJUgNDhNpVVIh4B3C04fu/By4vqC2SJClDxyEiInZExL6IOBoR\nZyNiZ4s6d0TEkYg4ERFPRMS1vWnusOh1Sh9U6m/nPt20pdW5ZfgXTVn786E26vxNF9fv5lx1qp1/\n/a9XZ63jqx0b9qcO/Whf7jU7Pa+ffTos/ZnzJGIzcBC4HUjNByPiFqrjHe4CrgaeAR6OiLGGan8P\nvLPh+8trZSVR1r90DBGtlbU/H26jzrNdXL+bc9UpQ0RrhojhDhFv6vSElNJD1P4JFBHRosoUcG9K\n6du1Op8BPgTcCnylVudJ4D0RsQ14Hfg3wJfXuO2m6i+znTa3TxaAmYKv9w8NX7f736Wd+6xV58g6\n92t1bquy9a6Tc+9O6jYfb27jaue3W76wSt3msk6+b/X1623UWWpRNtfmr0urlK9X1vx1q+/XK1/f\n4cOHz3399NNPd3RuY/2nn36aU6dOAfC2t71t1XpHjhw59/Xjjz/O2Nj5fxfNzp7v57m56s90/Pjx\nFcfqx59//vkVxxYWFpiZWfv358LCAg8++CAA27Zta3l8tWusdqxV+bFjx3jwwQdb3mPQ2vnvMqhr\ndnpeu326Vp1O+m218ob/zzat2+gORUorHia0f3LEWeDDKaV9te/fDLwB7KqX1cr3AltSSh9pKPt1\nqk8sAvgvKaX/vsZ9/h3wh9kNlSRJn0gpfaeXF+z4ScQ6xoALgWNN5ceA7Y0FKaU/A/6szes+DHwC\neBFY7K6JkiRtKJuAX6C9d6Ad6XWI6IuU0qtAT9OTJEkbyF/346K9nuI5D5wBtjaVbwVe7vG9JElS\ngXoaIlJKp4ADwA31strgyxvoUwqSJEnF6Ph1RkRsBq6kOiAS4N0RcRXwWkrpJeAbwN6IOEB1FsYU\ncCmwtyctliRJQ6Hj2RkR8QHgUVauEXFfSunWWp3bgc9RfY1xEPj3KaX/3X1zJUnSsOhqiqckSdq4\nCtsKvJfcVny0RMR3I+K1iHig6LaoOxHxzoh4NCKejYiDEfHRotukfBGxJSKeioiZiDgUEZ8quk3q\njYi4JCJejIivrF+74byyP4mobSv+f4EPUF3C7wBwXUrpZ4U2TNki4v3APwI+mVL6WNHtUb6I+CfA\n21NKhyJiK9Xfn/8spXSi4KYpQ22g/MUppcWIuITq2ugT/nlbfhHx+8A/BV5KKX2u3fNG4UmE24qP\nmJTSY8DPi26Hulf7fXmo9vUxqtPALyu2VcqVquoL/l1S+7XV9gcqkYi4kuqCkH/e6bmjECLcVlwq\ngYiYAC5IKR1dt7KGVu2VxkGgAnw1pfRa0W1S174G/B4ZgbDQEOG24qPF/hwtvezPiLgMuA/4rX63\nW631qj9TSgsppV8CfhH4RES8rbmOBqMXfVo753BK6fl6USdtKPpJhNuKj5Ze9KeGR0/6MyIuAv4E\nuDul9ON+N1qr6unvz5TSK7U6O/rVYK2rF336y8DHI+IFqk8kPhURn2+7BSmlofgAZ4GdTWVPAN9s\n+D6AnwKfayi7EDgMbAPeQnXP47cW/fNs9E9ufzYc+xXgj4r+Ofx035/ANPCFon8GP933J/B24C21\nr7cA/wd4T9E/j5/u/8ytHf8k8JVO7lv0k4hV1bYVnwB+UC9L1Z/yEeC6hrIzwO8CPwRmgK8lRwoP\nnXb7s1b3L4D/CfzbiKhExL8cZFu1vnb7MyKuB24GPhwRT9emBr5n0O3V2jr4/fku4PGIeBr4EdW/\noJ4dZFvVnk7+zO3GMO/i2a9txVWMTvrz1wbVKGVrqz9TSn/FcP85o6p2+/Mpqo/FNfza/jO3LqV0\nX6c3GdonEZIkabgNc4hwW/HRYn+OFvtztNifo2cgfTq0ISK5rfhIsT9Hi/05WuzP0TOoPi30XaXb\nio8W+3O02J+jxf4cPUPRpwVPSfkA1WkpZ5o+/6Ohzu3Ai8AJYD/w3qKn0vixPzfCx/4crY/9OXqf\nYejT0m/AJUmSijG0YyIkSdJwM0RIkqQshghJkpTFECFJkrIYIiRJUhZDhCRJymKIkCRJWQwRkiQp\niyFCkiRlMURIkqQshghJkpTFECFJkrL8f9FPwRKrd9/+AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xac665aac>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Distribution of word frequencies\n",
    "word_frequencies = [float(x) / 1000 for x in counter.values()]\n",
    "plt.hist(counter.values(), 500)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Inverse Document Frequency (IDF)\n",
    "\n",
    "One way to correct these frequencies is to look at a term’s inverse document frequency (idf), which decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take an English corpus containing thousands of documents to evaluate the inverse document frequency of english words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Brown english corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "5e2cb154-c68b-462d-98d3-dedcacefd6f9"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "[u'The', u'Fulton', u'County', u'Grand', u'Jury', u'said', u'Friday', u'an', u'investigation', u'of', u\"Atlanta's\", u'recent', u'primary', u'election', u'produced', u'``', u'no', u'evidence', u\"''\", u'that', u'any', u'irregularities', u'took', u'place', u'.']\n",
      "------\n",
      "[u'The', u'jury', u'further', u'said', u'in', u'term-end', u'presentments', u'that', u'the', u'City', u'Executive', u'Committee', u',', u'which', u'had', u'over-all', u'charge', u'of', u'the', u'election', u',', u'``', u'deserves', u'the', u'praise', u'and', u'thanks', u'of', u'the', u'City', u'of', u'Atlanta', u\"''\", u'for', u'the', u'manner', u'in', u'which', u'the', u'election', u'was', u'conducted', u'.']\n",
      "------\n",
      "[u'The', u'September-October', u'term', u'jury', u'had', u'been', u'charged', u'by', u'Fulton', u'Superior', u'Court', u'Judge', u'Durwood', u'Pye', u'to', u'investigate', u'reports', u'of', u'possible', u'``', u'irregularities', u\"''\", u'in', u'the', u'hard-fought', u'primary', u'which', u'was', u'won', u'by', u'Mayor-nominate', u'Ivan', u'Allen', u'Jr.', u'.']\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "brown_corpus = brown.sents()\n",
    "\n",
    "print \"------\"\n",
    "print brown_corpus[0]\n",
    "print \"------\"\n",
    "print brown_corpus[1]\n",
    "print \"------\"\n",
    "print brown_corpus[2]\n",
    "print \"------\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-process the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preprocessed_sentences = []\n",
    "for sentence in brown_corpus:\n",
    "    clean_sentence = preprocess_text(' '.join(sentence))\n",
    "    preprocessed_sentences.append(\" \".join(clean_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Inverse Document Frequency of english words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tf_idf = vectorizer.fit(preprocessed_sentences)\n",
    "idf = vectorizer.idf_\n",
    "idf = dict(zip(vectorizer.get_feature_names(), idf))\n",
    "sorted_idf = sorted(idf.items(), key=operator.itemgetter(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most common words accross English documents (Low IDF value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'said', 4.3852975302665964),\n",
      " (u'time', 4.4300536177651608),\n",
      " (u'year', 4.5248792623100664),\n",
      " (u'new', 4.6178830452789867),\n",
      " (u'like', 4.6509111464338577),\n",
      " (u'use', 4.7118296327808586),\n",
      " (u'state', 4.7182743382235017),\n",
      " (u'man', 4.7556002958361878),\n",
      " (u'make', 4.8518057308480245),\n",
      " (u'work', 4.9075163378620292),\n",
      " (u'day', 4.9637560561849057),\n",
      " (u'way', 5.0301941458951411),\n",
      " (u'look', 5.0311759820073982),\n",
      " (u'mani', 5.0600768474951545),\n",
      " (u'come', 5.120511901840799),\n",
      " (u'long', 5.1791245854827492),\n",
      " (u'good', 5.1848356232782642),\n",
      " (u'peopl', 5.1951984103138109),\n",
      " (u'world', 5.238966484094175),\n",
      " (u'place', 5.2401764055968885)]\n"
     ]
    }
   ],
   "source": [
    "pprint(sorted_idf[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Most rare words accross English documents (High IDF value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'vanc', 11.263623998557922),\n",
      " (u'pigen', 11.263623998557922),\n",
      " (u'northerli', 11.263623998557922),\n",
      " (u'xenon', 11.263623998557922),\n",
      " (u'downpay', 11.263623998557922),\n",
      " (u'boor', 11.263623998557922),\n",
      " (u'worriedli', 11.263623998557922),\n",
      " (u'theon', 11.263623998557922),\n",
      " (u'recontamin', 11.263623998557922),\n",
      " (u'slipp', 11.263623998557922),\n",
      " (u'horsewoman', 11.263623998557922),\n",
      " (u'libello', 11.263623998557922),\n",
      " (u'daffodil', 11.263623998557922),\n",
      " (u'mudwagon', 11.263623998557922),\n",
      " (u'bratwurst', 11.263623998557922),\n",
      " (u'sweatband', 11.263623998557922),\n",
      " (u'contradictori', 11.263623998557922),\n",
      " (u'footwork', 11.263623998557922),\n",
      " (u'piousli', 11.263623998557922),\n",
      " (u'warranti', 11.263623998557922)]\n"
     ]
    }
   ],
   "source": [
    "sorted_idf.reverse()\n",
    "pprint(sorted_idf[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Term Frequency - Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a measure that quantifies how important a word is within a document (TF) and another one that quantifies how common a word is accross the language (IDF), we can combine them into\n",
    "\n",
    "**TF-IDF = TF * IDF**\n",
    "\n",
    "This measure (TF-IDF) attempts to find the words that are important within the document (i.e., high frequency), but not too common across the documents (i.e. in english language in general)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF of Book reviews\n",
    "\n",
    "We can now compute the TF-IDF values for each word within a particular category.\n",
    "\n",
    "Each document can then be represented as a **vector** of TF-IDF values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "2ac4ecac-050f-4913-84be-3541aee398dc"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'book', 0.72413773358744526),\n",
      " (u'read', 0.27538827141993177),\n",
      " (u'stori', 0.12298503627368282),\n",
      " (u'like', 0.11614221200726135),\n",
      " (u'charact', 0.097071552969261496),\n",
      " (u'author', 0.09564121094383489),\n",
      " (u'time', 0.090579081060064723),\n",
      " (u'good', 0.083520708978920818),\n",
      " (u'work', 0.080664457047949445),\n",
      " (u'realli', 0.077258511589321233),\n",
      " (u'make', 0.076904639717952902),\n",
      " (u'write', 0.076498786335732594),\n",
      " (u'novel', 0.075527966727990711),\n",
      " (u'mani', 0.074866738623109552),\n",
      " (u'use', 0.073139167281455461)]\n"
     ]
    }
   ],
   "source": [
    "# Book review document \n",
    "book_doc = \" \".join(book_reviews_prepro)\n",
    "# Compute TF-IDF\n",
    "result = vectorizer.transform([book_doc])\n",
    "feature_names = tf_idf.get_feature_names()\n",
    "tfidf = []\n",
    "for col in result.nonzero()[1]:\n",
    "    tfidf.append((feature_names[col],result[0, col]))\n",
    "sorted_tfidf = sorted(tfidf, key=lambda x: x[1])\n",
    "sorted_tfidf.reverse()\n",
    "pprint(sorted_tfidf[0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "7c6908b6-4b77-4dee-ad11-95d277ba5298"
    }
   },
   "source": [
    "#### TF-IDF of DVD reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'movi', 0.56391529121228801),\n",
      " (u'film', 0.38891147315815311),\n",
      " (u'like', 0.15260879665206684),\n",
      " (u'watch', 0.15244555227445547),\n",
      " (u'charact', 0.12419015633987199),\n",
      " (u'good', 0.1161426936296634),\n",
      " (u'realli', 0.1046114478940887),\n",
      " (u'stori', 0.10395677590874956),\n",
      " (u'time', 0.10175331054448554),\n",
      " (u'scene', 0.10092270494695392),\n",
      " (u'great', 0.099987215691401232),\n",
      " (u'make', 0.098152862448165096),\n",
      " (u'love', 0.097762108931150685),\n",
      " (u'video', 0.088369938069422893),\n",
      " (u'star', 0.08040999372700279)]\n"
     ]
    }
   ],
   "source": [
    "# DVD review document \n",
    "dvd_doc = \" \".join(dvd_reviews_prepro)\n",
    "\n",
    "# Compute TF-IDF\n",
    "result = vectorizer.transform([dvd_doc])\n",
    "feature_names = tf_idf.get_feature_names()\n",
    "tfidf = []\n",
    "for col in result.nonzero()[1]:\n",
    "    tfidf.append((feature_names[col],result[0, col]))\n",
    "sorted_tfidf = sorted(tfidf, key=lambda x: x[1])\n",
    "sorted_tfidf.reverse()\n",
    "pprint(sorted_tfidf[0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF of Electronics reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'use', 0.27024762619575893),\n",
      " (u'work', 0.23288635223217749),\n",
      " (u'sound', 0.16652057246502772),\n",
      " (u'phone', 0.16078096379166854),\n",
      " (u'product', 0.15785345960184044),\n",
      " (u'cabl', 0.15133594177842638),\n",
      " (u'bought', 0.15021833741088095),\n",
      " (u'buy', 0.1471263348755161),\n",
      " (u'good', 0.14105254703576869),\n",
      " (u'qualiti', 0.13207769243477641),\n",
      " (u'great', 0.12768321699572702),\n",
      " (u'card', 0.12235712791229919),\n",
      " (u'batteri', 0.12077664662645921),\n",
      " (u'purchas', 0.11729265849911978),\n",
      " (u'problem', 0.1155136289390558)]\n"
     ]
    }
   ],
   "source": [
    "# Electronics review document \n",
    "elec_doc = \" \".join(elec_reviews_prepro)\n",
    "\n",
    "# Compute TF-IDF\n",
    "result = vectorizer.transform([elec_doc])\n",
    "feature_names = tf_idf.get_feature_names()\n",
    "tfidf = []\n",
    "for col in result.nonzero()[1]:\n",
    "    tfidf.append((feature_names[col],result[0, col]))\n",
    "sorted_tfidf = sorted(tfidf, key=lambda x: x[1])\n",
    "sorted_tfidf.reverse()\n",
    "pprint(sorted_tfidf[0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF of Kitchen appliance reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'use', 0.32542916525553262),\n",
      " (u'pan', 0.25109698285940663),\n",
      " (u'coffe', 0.16045749277750573),\n",
      " (u'product', 0.14255764667841309),\n",
      " (u'time', 0.14241932457146358),\n",
      " (u'clean', 0.13878441135843655),\n",
      " (u'make', 0.13817737071500366),\n",
      " (u'great', 0.13762194092689842),\n",
      " (u'work', 0.13751335740829032),\n",
      " (u'buy', 0.13361253873960222),\n",
      " (u'like', 0.13138950484896789),\n",
      " (u'cook', 0.13065563057038931),\n",
      " (u'set', 0.12448419642698419),\n",
      " (u'bought', 0.1215814737983129),\n",
      " (u'purchas', 0.10788817015898589)]\n"
     ]
    }
   ],
   "source": [
    "# kitchen appliance review document \n",
    "kitch_doc = \" \".join(kitch_reviews_prepro)\n",
    "\n",
    "# Compute TF-IDF\n",
    "result = vectorizer.transform([kitch_doc])\n",
    "feature_names = tf_idf.get_feature_names()\n",
    "tfidf = []\n",
    "for col in result.nonzero()[1]:\n",
    "    tfidf.append((feature_names[col],result[0, col]))\n",
    "sorted_tfidf = sorted(tfidf, key=lambda x: x[1])\n",
    "sorted_tfidf.reverse()\n",
    "pprint(sorted_tfidf[0:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF allows us to represent each text document in a mathematical way that truly quantify its content. Each document can be represented as a vector of real value tf-idf weights.\n",
    "\n",
    "TF-IDF representation can solve several problems:\n",
    "* Document comparison for plagiarism\n",
    "* Text classification\n",
    "* Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** Given a new review or comment as input, we would like to automatically detect the category it belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input review\n",
    "input_review= \"\"\"Yes. Wild things is what I recommend for our jaded eyes. \n",
    "Aren't we sick of all the crowd pleasing PG-13 shows which are neither sexy or \n",
    "action packed - most of all with hardly a plot? Wild Things is sex sex sex but \n",
    "with witty capital H humor and a twisted story\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing of the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u\"ye wild thing recommend jade eye n't sick crowd pleas pg-13 show neither sexi action pack hardli plot wild thing sex sex sex witti capit h humor twist stori\"\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing of input\n",
    "input_prepro = \" \".join(preprocess_text(input_review))\n",
    "pprint(input_prepro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computation of tf-idf representation of the text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'sex', 0.50349964676856307),\n",
      " (u'wild', 0.35467452329040938),\n",
      " (u'sexi', 0.24438368359801532),\n",
      " (u'thing', 0.24437170164766803),\n",
      " (u'jade', 0.23288657355646061),\n",
      " (u'witti', 0.21514082899425271),\n",
      " (u'twist', 0.17851180207326731),\n",
      " (u'plot', 0.17811343915619088),\n",
      " (u'sick', 0.17733726164520469),\n",
      " (u'humor', 0.17586086903669326),\n",
      " (u'13', 0.17447539063307865),\n",
      " (u'pack', 0.17414194872331093),\n",
      " (u'crowd', 0.1647965976665893),\n",
      " (u'hardli', 0.16393908758033063),\n",
      " (u'capit', 0.16291119584067593),\n",
      " (u'pleas', 0.16271113303674639),\n",
      " (u'recommend', 0.1600823415831156),\n",
      " (u'ye', 0.15589040945414548),\n",
      " (u'stori', 0.14701115835468007),\n",
      " (u'action', 0.13791846281458209),\n",
      " (u'eye', 0.12801202819671303)]\n"
     ]
    }
   ],
   "source": [
    "result = vectorizer.transform([input_prepro])\n",
    "feature_names = tf_idf.get_feature_names()\n",
    "tfidf = []\n",
    "for col in result.nonzero()[1]:\n",
    "    tfidf.append((feature_names[col],result[0, col]))\n",
    "sorted_tfidf = sorted(tfidf, key=lambda x: x[1])\n",
    "sorted_tfidf.reverse()\n",
    "pprint(sorted_tfidf[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computation of similarity between input and our categories\n",
    "\n",
    "We know have a mathematical representation (vector) for both our input text and the categories (see point 4)\n",
    "\n",
    "In mathematics, the similarity between two vectors can be measured by the cosine of the angle between the two vectors. This measure is know has the **cosine similarity**.\n",
    "\n",
    "Two vectors with similar values will have a small angle, and thus a cosine value near zero. On the other hand, two very different vectors will have a large angle between them and thus a cosine value close to 1.\n",
    "\n",
    "Let us try to compare our input with each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity with Books: \t\t 0.071508\n",
      "similarity with DVD: \t\t 0.082246\n",
      "similarity with Elec: \t\t 0.041240\n",
      "similarity with Kitchen app: \t 0.042110\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF vectors for each category\n",
    "book_vector = vectorizer.transform([book_doc]) # Books\n",
    "dvd_vector = vectorizer.transform([dvd_doc]) # DVD\n",
    "elec_vector = vectorizer.transform([elec_doc]) # Electronics\n",
    "kitch_vector = vectorizer.transform([kitch_doc]) # Kitchen Appliances\n",
    "\n",
    "# TF-IDF vector of our input\n",
    "input_vector = vectorizer.transform([input_prepro])\n",
    "\n",
    "# Cosine similarities\n",
    "print 'similarity with Books: \\t\\t %f' % (cosine_similarity(input_vector,book_vector)[0][0])\n",
    "print 'similarity with DVD: \\t\\t %f' % (cosine_similarity(input_vector,dvd_vector)[0][0])\n",
    "print 'similarity with Elec: \\t\\t %f' % (cosine_similarity(input_vector,elec_vector)[0][0])\n",
    "print 'similarity with Kitchen app: \\t %f' % (cosine_similarity(input_vector,kitch_vector)[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try another one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity with Books: \t\t 0.117077\n",
      "similarity with DVD: \t\t 0.221470\n",
      "similarity with Elec: \t\t 0.138294\n",
      "similarity with Kitchen app: \t 0.144531\n"
     ]
    }
   ],
   "source": [
    "# Input review\n",
    "input_review= \"\"\"This is a real \"in your face\" drama that has been all but forgotten about.  Hopefully the rumors of the remake are true.  \n",
    "A couple of key things without rehashing the plot....great dialogue, especially from Hal Holbrook and the other judges.  When they were looking to fill a vacancy in their ranks and a name is brought up, they disdainfully tore up the potential nominee....\"he's a lightweight....I'm sure he's nice to his cocker-spanial, but that's just not good enough\".  Great stuff.  And when Holbrook finally explains it all to Michael Douglas...\"you are depressingly familiar\".  I love that line.\n",
    "\n",
    "Yes, there are some weak plot points in spots, but overall this movie presents complex issues without clear answers.  You have to ask yourself...what would you do?  The Doctor who's little boy was killed says it all.....\"You don't escape so easily\".  That's what makes this so rivoting....no black and white. \n",
    "\n",
    "Go buy it....its time to get your fingernails dirty.\"\"\"\n",
    "\n",
    "# Preprocessing of input\n",
    "input_prepro = \" \".join(preprocess_text(input_review))\n",
    "\n",
    "# TF-IDF vector of our input\n",
    "input_vector = vectorizer.transform([input_prepro])\n",
    "\n",
    "# Cosine similarities\n",
    "print 'similarity with Books: \\t\\t %f' % (cosine_similarity(input_vector,book_vector)[0][0])\n",
    "print 'similarity with DVD: \\t\\t %f' % (cosine_similarity(input_vector,dvd_vector)[0][0])\n",
    "print 'similarity with Elec: \\t\\t %f' % (cosine_similarity(input_vector,elec_vector)[0][0])\n",
    "print 'similarity with Kitchen app: \\t %f' % (cosine_similarity(input_vector,kitch_vector)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:** Given a new review or comment as input, we would like to assess the sentiment associated to this review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computation of tf-idf representation of positive reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge positive reviews together\n",
    "with open('data/sorted_data_acl/books/positive_text.review', 'r') as myfile:\n",
    "    pos_book = myfile.read().decode(\"utf-8\")\n",
    "with open('data/sorted_data_acl/dvd/positive_text.review', 'r') as myfile:\n",
    "    pos_dvd = myfile.read().decode(\"utf-8\")\n",
    "with open('data/sorted_data_acl/electronics/positive_text.review', 'r') as myfile:\n",
    "    pos_elec = myfile.read().decode(\"utf-8\")\n",
    "with open('data/sorted_data_acl/kitchen_&_housewares/positive_text.review', 'r') as myfile:\n",
    "    pos_kitch = myfile.read().decode(\"utf-8\")\n",
    "pos_reviews = pos_book + pos_dvd + pos_elec + pos_kitch\n",
    "\n",
    "# pre-processing of positive reviews\n",
    "pos_reviews_prepro = preprocess_text(pos_reviews.replace(\"'\",' '))\n",
    "pos_doc = \" \".join(pos_reviews_prepro)\n",
    "\n",
    "# TF-IDF vector of positive reviews\n",
    "pos_vector = vectorizer.transform([pos_doc]) # Books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computation of tf-idf representation of negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge negative reviews together\n",
    "with open('data/sorted_data_acl/books/negative_text.review', 'r') as myfile:\n",
    "    neg_book = myfile.read().decode(\"utf-8\")\n",
    "with open('data/sorted_data_acl/dvd/negative_text.review', 'r') as myfile:\n",
    "    neg_dvd = myfile.read().decode(\"utf-8\")\n",
    "with open('data/sorted_data_acl/electronics/negative_text.review', 'r') as myfile:\n",
    "    neg_elec = myfile.read().decode(\"utf-8\")\n",
    "with open('data/sorted_data_acl/kitchen_&_housewares/negative_text.review', 'r') as myfile:\n",
    "    neg_kitch = myfile.read().decode(\"utf-8\")\n",
    "neg_reviews = neg_book + neg_dvd + neg_elec + neg_kitch\n",
    "\n",
    "# pre-processing of positive reviews\n",
    "neg_reviews_prepro = preprocess_text(neg_reviews.replace(\"'\",' '))\n",
    "neg_doc = \" \".join(neg_reviews_prepro)\n",
    "\n",
    "# TF-IDF vector of positive reviews\n",
    "neg_vector = vectorizer.transform([neg_doc]) # Books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input review\n",
    "input_review= \"\"\"Interference from other electronics is a severe problem - \n",
    "I had to return this item for a refund.  If you can locate it several feet from any other \n",
    "electronics, it might work for you, but who wants a phone that you cannot place on your desktop, near a computer\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing of the input¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u'interfer electron sever problem return item refund locat sever feet electron might work want phone place desktop near comput'\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing of input\n",
    "input_prepro = \" \".join(preprocess_text(input_review))\n",
    "pprint(input_prepro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computation of tf-idf representation of input reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TF-IDF vector of our input\n",
    "input_vector = vectorizer.transform([input_prepro])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computation of similarity between input and positive/negative reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity with Positive: \t\t 0.098255\n",
      "similarity with Negative: \t\t 0.122856\n"
     ]
    }
   ],
   "source": [
    "# Cosine similarities\n",
    "print 'similarity with Positive: \\t\\t %f' % (cosine_similarity(input_vector,pos_vector)[0][0])\n",
    "print 'similarity with Negative: \\t\\t %f' % (cosine_similarity(input_vector,neg_vector)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nbpresent": {
   "slides": {
    "70827a56-96fa-45b1-8cd1-752cfbe59cd8": {
     "id": "70827a56-96fa-45b1-8cd1-752cfbe59cd8",
     "prev": null,
     "regions": {
      "eefdf89d-1555-49eb-b647-651ee92aa455": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "id": "eefdf89d-1555-49eb-b647-651ee92aa455"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
